from transformers import GPT2Tokenizer, GPT2LMHeadModel
from numpy import array
from numpy.random import choice
import tweepy

model = GPT2LMHeadModel.from_pretrained('model') #download gpt2 model that is supposed to generate text, name it pytorch_model.bin and put it into a 'model' folder
tokenizer = GPT2Tokenizer.from_pretrained('model')

#probs and words are starting tokens to guide GPT2 and generate better text. 
#Obtained by getting first tokens from each example (weighted by frequency)
probs = [0.14602589844604397, 0.07557440826548237, 0.054374940095404944, 0.0366063156032116, 0.03609390091349897, 0.031622253297517534, 0.026472301344074772, 0.021777009906683988, 0.01698955755387726, 0.01608638058280104, 0.015233584912152204, 0.014930068297382373, 0.014749432903167131, 0.013314180247089549, 0.012707147017549887, 0.012251257689292366, 0.010878674455697627, 0.010125412573698006, 0.00951346409533616, 0.008466516096211076, 0.007978677650609363, 0.007778381261105386, 0.007571940810573678, 0.007452746026635728, 0.007379017294302976, 0.006714229891102657, 0.006588891046136978, 0.006501645379543221, 0.006392281093249639, 0.005886010464564739, 0.005511222741873247, 0.00489681663910031, 0.004879613268222668, 0.004871011582783846, 0.004733384615762708, 0.0046043593341803905, 0.00457486784124729, 0.004519571291997726, 0.004481478113625804, 0.004413893442320781, 0.004331563024549207, 0.004241859733544359, 0.004066139588151298, 0.00391376687466361, 0.0037343602926539123, 0.0036323688795936045, 0.0035758435181384943, 0.003564784208288582, 0.0033325387014404113, 0.003226860851763466, 0.003180165987952723, 0.003146988058402984, 0.003110123692236608, 0.0030228780256428512, 0.0030069034669707546, 0.0030056746547652085, 0.0029049120539104472, 0.0028729629365662544, 0.0028619036267163414, 0.0028053782652612312, 0.0028004630164390476, 0.002745166467189483, 0.002732878345134025, 0.0026947851667621027, 0.0026947851667621027, 0.0025264378946023174, 0.002512920960341313, 0.0024613108477083865, 0.002413387171692097, 0.00234703131259262, 0.002324912692892794, 0.0023027940731929685, 0.0022647008948210464, 0.002249955148354496, 0.0021577942329385554, 0.0021455061108830967, 0.0021074129325111745, 0.002101268871483445, 0.002074235002961436, 0.0020484299466449725, 0.0019144894162404721, 0.001818642064207894, 0.0017522862051084168, 0.0017387692708474122, 0.0016957608436533067, 0.0016269473601427376, 0.0016060575526484577, 0.0016048287404429118, 0.0015532186278099853, 0.0015470745667822557, 0.0015003797029715126, 0.0014819475198883244, 0.0014696593978328657, 0.0014499984025441318, 0.0014340238438720354, 0.0014291085950498517, 0.0014033035387333885, 0.0013861001678557464, 0.0013516934261004617, 0.00132834599419509, 0.001319744308756269, 0.0012767358815621635, 0.0012583036984789753, 0.0012509308252457, 0.0012386427031902414, 0.0012152952712848697, 0.0011661427830630348, 0.0011575410976242136, 0.0011538546610075759, 0.001145252975568755, 0.0011108462338134705, 0.001086269989702553, 0.0010813547408803695, 0.0010764394920581859, 0.0010764394920581859, 0.0010629225577971813, 0.0010506344357417226, 0.001046947999125085, 0.0010432615625084473, 0.0010223717550141675, 0.0010113124451642547, 0.0010051683841365252, 0.0010051683841365252, 0.0009842785766422453, 0.0009805921400256078, 0.000978134515614516, 0.0009633887691479655, 0.0009572447081202361, 0.000938812525037048, 0.0009363549006259563, 0.0009338972762148645, 0.0009302108395982269, 0.0009130074687205847, 0.0008994905344595801, 0.000889660036815213, 0.0008847447879930296, 0.0008773719147597543, 0.0008564821072654744, 0.0008527956706488368, 0.0008454227974155616, 0.0008368211119767405, 0.0008368211119767405, 0.0008331346753601028, 0.0008269906143323735, 0.0008245329899212817, 0.0008048719946325477, 0.0007999567458103642, 0.0007999567458103642, 0.0007839821871382679, 0.0007680076284661715, 0.0007680076284661715, 0.0007458890087663457, 0.0007385161355330705, 0.0007372873233275247, 0.0007360585111219788, 0.0007360585111219788, 0.0007274568256831576, 0.0007249992012720659, 0.00072377038906652, 0.0007176263280387906, 0.000713939891422153, 0.0006856772106945979, 0.000684448398489052, 0.0006795331496668685, 0.0006758467130502309, 0.0006611009665836805, 0.0006549569055559511, 0.0006524992811448593, 0.0006475840323226758, 0.0006475840323226758, 0.0006426687835004922, 0.0006389823468838547, 0.0006340670980616712, 0.0006340670980616712, 0.0006316094736505794, 0.0006303806614450336, 0.0006303806614450336, 0.0006291518492394876, 0.00062546541262285, 0.0006217789760062125, 0.0006217789760062125, 0.0006205501638006665, 0.0006205501638006665, 0.0006205501638006665, 0.000616863727184029, 0.000616863727184029, 0.0006156349149784831, 0.0006144061027729371, 0.0006082620417452078, 0.0006008891685119326, 0.0005984315441008407, 0.0005959739196897491, 0.0005873722342509279, 0.0005849146098398363, 0.0005726264877843775, 0.0005689400511677398, 0.0005677112389621939, 0.0005664824267566481, 0.0005652536145511022, 0.0005652536145511022, 0.0005603383657289187, 0.0005566519291122812, 0.0005443638070568224, 0.0005443638070568224, 0.0005431349948512765, 0.0005394485582346388, 0.0005345333094124554, 0.0005308468727958177, 0.0005296180605902719, 0.0005247028117680883, 0.0005222451873569966, 0.000518558750740359, 0.000518558750740359, 0.0005173299385348131, 0.0005173299385348131, 0.0005173299385348131, 0.0005148723141237214, 0.0005148723141237214, 0.0005136435019181755, 0.0005124146897126297, 0.0005050418164793543, 0.0005050418164793543, 0.0005038130042738085, 0.0005038130042738085, 0.000498897755451625, 0.0004976689432460791, 0.0004927536944238956, 0.0004927536944238956, 0.0004915248822183497, 0.000489067257807258, 0.00048660963339616623, 0.0004853808211906204, 0.00047800794795734513, 0.00047677913575179927, 0.00047677913575179927, 0.0004718638869296158, 0.0004644910136963405, 0.00046326220149079463, 0.000459575764874157, 0.00045834695266861115, 0.0004522028916408818, 0.0004509740794353359, 0.0004448300184076065, 0.0004448300184076065, 0.000439914769585423, 0.00043868595737987716, 0.00043868595737987716, 0.00043868595737987716, 0.00043499952076323954, 0.0004325418963521478, 0.0004251690231188725, 0.00042394021091332666, 0.00042394021091332666, 0.0004227113987077808, 0.00042148258650223495, 0.00042025377429668904, 0.00041410971326895964, 0.00041410971326895964, 0.0004067368400356844, 0.0004042792156245926, 0.00040305040341904677, 0.00040059277900795506, 0.0003993639668024092, 0.0003956775301857715, 0.0003907622813635881, 0.0003883046569524963, 0.0003870758447469504, 0.0003870758447469504, 0.00038461822033585864, 0.0003833894081303128, 0.0003797029715136752, 0.0003784741593081293, 0.0003784741593081293, 0.00037724534710258345, 0.00037601653489703754, 0.00037601653489703754, 0.0003747877226914917, 0.0003698724738693082, 0.0003698724738693082, 0.0003698724738693082, 0.00036741484945821643, 0.0003661860372526706, 0.00036495722504712466, 0.0003612707884304871, 0.0003612707884304871, 0.0003588131640193953, 0.0003551267274027577, 0.0003551267274027577, 0.0003502114785805742, 0.0003440674175528448, 0.00033792335652511543, 0.0003366945443195696, 0.0003366945443195696, 0.0003366945443195696, 0.0003342369199084778, 0.0003342369199084778, 0.0003342369199084778, 0.0003342369199084778, 0.000333008107702932, 0.0003317792954973861, 0.0003317792954973861, 0.00033055048329184023, 0.0003293216710862943, 0.00032809285888074847, 0.00032686404667520255, 0.00032686404667520255, 0.0003256352344696567, 0.0003244064222641108, 0.0003244064222641108, 0.000323177610058565, 0.0003207199856474732, 0.00031826236123638145, 0.0003170335490308356, 0.0003121183002086521, 0.00031088948800310625, 0.00031088948800310625, 0.00030966067579756034, 0.0003047454269753768, 0.0003047454269753768, 0.00030105899035873923, 0.0002998301781531934, 0.00029860136594764747, 0.00029860136594764747, 0.00029860136594764747, 0.0002961437415365557, 0.0002961437415365557, 0.00029491492933100984, 0.0002912284927143722, 0.0002912284927143722, 0.0002850844316866428, 0.0002850844316866428, 0.0002850844316866428, 0.00028385561948109697, 0.0002826268072755511, 0.0002826268072755511, 0.00028016918286445934, 0.0002764827462478217, 0.0002764827462478217, 0.0002764827462478217, 0.0002764827462478217, 0.0002764827462478217, 0.00027525393404227586, 0.00027402512183672995, 0.00027402512183672995, 0.0002703386852200924, 0.0002703386852200924, 0.00026910987301454647, 0.0002678810608090006, 0.00026665224860345475, 0.000264194624192363, 0.0002629658119868171, 0.0002592793753701795, 0.0002592793753701795, 0.0002592793753701795, 0.0002592793753701795, 0.0002592793753701795, 0.00025682175095908773, 0.0002555929387535419, 0.00025436412654799597, 0.00025436412654799597, 0.00025436412654799597, 0.00025190650213690425, 0.0002506776899313584, 0.0002506776899313584, 0.0002482200655202666, 0.0002469912533147207, 0.000244533628903629, 0.00024207600449253723, 0.00023961838008144552, 0.00023961838008144552, 0.00023961838008144552, 0.00023838956787589964, 0.00023716075567035375, 0.00023716075567035375, 0.00023716075567035375, 0.0002359319434648079, 0.0002359319434648079, 0.0002359319434648079, 0.000234703131259262, 0.000234703131259262, 0.000234703131259262, 0.00023224550684817024, 0.00023101669464262439, 0.0002297878824370785, 0.00022855907023153264, 0.00022855907023153264, 0.0002261014458204409, 0.0002261014458204409, 0.00022487263361489502, 0.00022487263361489502, 0.00022364382140934914, 0.00022364382140934914, 0.00022364382140934914, 0.00022241500920380325, 0.0002211861969982574, 0.0002211861969982574, 0.0002199573847927115, 0.0002199573847927115, 0.00021872857258716563, 0.00021872857258716563]
words = ["The", "In", "This", "I", "It", "We", "As", "But", "And", "However", "A", "For", "These", "*", "There", "At", "B", "To", "On", "While", "If", "That", "Let", "With", "So", "Our", "First", "They", "What", "One", "Moreover", "\u2022", "Second", "Since", "Indeed", "\ufffd", "Today", "L", "When", "Some", "Although", "By", "[", "All", "Therefore", "Thus", "Over", "According", "Such", "Given", "An", "Of", "Finally", "After", "Financial", "From", "Furthermore", "F", "More", "Many", "My", "Another", "C", "Even", "Third", "H", "You", "During", "How", "Thank", "Mon", "G", "Nevertheless", "Looking", "Under", "Despite", "Not", "R", "Now", "(", "Yet", "Also", "Most", "T", "S", "Central", "Economic", "He", "Before", "Against", "Both", "Overall", "Further", "Con", "Meanwhile", "M", "Last", "Here", "Ex", "Because", "Other", "P", "Global", "E", "N", "1", "Similarly", "Why", "Then", "Among", "Conclusion", "Bank", "Following", "Recent", "Its", "Those", "html", "Instead", "Is", "Reg", "Un", "Having", "No", "2", "D", "Nonetheless", "Based", "Through", "Introduction", "Invest", "au", "New", "Their", "Good", "Rather", "Secondly", "Once", "Without", "Mr", "Turn", "Only", "Just", "Two", "Part", "Chart", "Currently", "Besides", "Fourth", "Several", "Cons", "Develop", "W", "Unfortunately", "Clearly", "Allow", "Specifically", "Do", "Within", "Credit", "Much", "Perhaps", "House", "Your", "O", "Business", "Dom", "Market", "J", "Together", "Well", "Firstly", "Regarding", "Are", "Jean", "International", "Any", "Going", "Still", "China", "***", "Taking", "Real", "Recently", "Interest", "Where", "3", "St", "Each", "Private", "Or", "Like", "High", "Re", "Should", "V", "Capital", "Likewise", "Dist", "Sub", "K", ",", "Though", "Graph", "Cor", "Spe", "Equ", "Low", "Apart", "Super", "Mac", "https", "Page", ")", "Govern", "Pro", "Dear", "Price", "Ben", "Y", "European", "Public", "Along", "Pr", "Ad", "Beyond", "Nor", "Comp", "Foreign", "Import", "Me", "Euro", "Again", "Bas", "Policy", "India", "Res", "Rec", "eu", "Japan", "Unlike", "Ultimately", "Thanks", "Current", "Europe", "Whether", "Obviously", "People", "Trans", "Consumer", "Compared", "Yes", "Lastly", "Three", "Mark", "Higher", "Strong", "Additionally", "Struct", "\u2013", "Certainly", "Between", "Ar", "Z", "Fortunately", "Sw", "His", "Due", "Employ", "Mal", "Can", "Next", "Count", "Sh", "Does", "Ref", "-", "Until", "Emer", "Large", "Sign", "Gl", "4", "Will", "MAS", "Long", "Am", "Naturally", "Money", "Being", "Im", "South", "Mario", "Econom", "Non", "Islamic", "Please", "Increased", "En", "Prior", "Government", "Great", "Take", "Asia", "Pay", "Fif", "Others", "Rem", "Ind", "Technology", "Need", "Every", "Data", "Throughout", "Ann", "Key", "Moving", "Changes", "U", "Inst", "Incre", "Hist", "Total", "Fin", "Lower", "Liquid", "Almost", "Experience", "Similar", "Progress", "Cont", "Sing", "Digital", "Canada", "National", "Out", "Co", "See", "Tr", "Building", "Cl", "Considering", "Se", "Otherwise", "Earlier", "Th", "Consider", "Trade", "Later", "Acc", "Various", "About", "Man", "Chall", "App", "Conf", "b", "Starting", "Less", "Inter", "Small", "Techn", "Fund", "Put", "Product", "Ass", "Contin", "Notes", "Est", "David", "Pre", "Up", "Sound", "Cross", "Iss", "Dep", "Sometimes", "Alan", "Whereas", "Ins", "May", "Work", "sp", "Examples", "History", "Generally", "Deb", "Amid", "Indust", "Stre", "Dis", "Note", "Germany", "Effective", "Information", "Sur", "Broad", "Dr", "Back", "Major", "Figure"]

probs = array(probs)
probs /= sum(probs

def check(text): #fitler out bad text
        bad_strings = ["www","*  *  *", "BIS Review"]
        for bs in bad_strings:
            if bs in text:
                return True
        if text.endswith("''''''"):
            return True
        elif len(text) < 140:
            return True
        else:
            return False
    
    def get_tweet():
        keep_drawing = True 
        while keep_drawing:
            draw = choice(words, 1, p=probs)
            start_token = draw[0]
            encoded_input = tokenizer(start_token, return_tensors='pt')
            output = model.generate(**encoded_input,max_length = 96,top_p=0.95,top_k=50,do_sample=True)
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
            sentences = generated_text.split(".")
            sentences = [x+"." for x in sentences]
            tot_len = 0
            new_sentence = ""
            for sent in sentences:
                if tot_len + len(sent) < 280:
                    tot_len += len(sent)
                    new_sentence += sent
                else:
                    break

            keep_drawing = check(new_sentence)

        return(new_sentence)


def handler(event, context):
    
    consumer_key = ""
    consumer_secret = ""
    access_token = ""
    access_token_secret = ""

    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth)

    tweet = get_tweet()
    
    api.update_status(tweet)
    
    return tweet, len(tweet)


