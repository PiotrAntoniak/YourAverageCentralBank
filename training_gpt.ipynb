{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89369455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import json\n",
    "import tweepy\n",
    "\n",
    "import pandas\n",
    "from random import random\n",
    "import copy\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33703302-3af0-4af1-ac2b-7dea2eb6008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c272c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book(title): #load a singel book from a memory\n",
    "    with open(\"text/{}\".format(title), \"r\", encoding = \"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up text\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.split(\"* * *\")[-1]\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\"\\x0c\",\"\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [\"\" if s.startswith(\"§\") else s for s in sentences]\n",
    "    #sentences = [s.lower() for s in sentences]\n",
    "    sentences = [re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", s) for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 10]\n",
    "    sentences = [s for s in sentences if len(s)/len(s.split()) > 4]\n",
    "    sentences = [s+\" \" for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51326f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text. Follows RoBERTa paper (adapted for text generation problem - i.e. no random words)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer = tokenizer, max_length = 128):\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tok = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length = max_length)\n",
    "        tokenized_sentences.append(tok)\n",
    "    \n",
    "    tokenized_sentence = tokenized_sentences[0]  \n",
    "    \n",
    "    for tokenized in tokenized_sentences[1:]:\n",
    "        \n",
    "        size_tokenized_sentence = tokenized_sentence[\"input_ids\"].shape[1]\n",
    "        size_tokenized = tokenized[\"input_ids\"].shape[1]\n",
    "        \n",
    "        if size_tokenized_sentence + size_tokenized < max_length:\n",
    "            tokenized_sentence = {'input_ids': torch.hstack([tokenized_sentence['input_ids'],\n",
    "                                               tokenized['input_ids']]),\n",
    "                                 'attention_mask':torch.hstack([tokenized_sentence['attention_mask'],\n",
    "                                               tokenized['attention_mask']])\n",
    "                                 }\n",
    "        else:\n",
    "            target = torch.ones(1,  max_length)\n",
    "            target[:,  :tokenized_sentence['input_ids'].shape[1]] = tokenized_sentence['input_ids']\n",
    "            tokenized_sentence['input_ids'] = target.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            target = torch.zeros(1,  max_length)\n",
    "            target[:, :tokenized_sentence['attention_mask'].shape[1]] = tokenized_sentence['attention_mask']\n",
    "            tokenized_sentence['attention_mask'] = target.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            tokenized_sentence_masked = copy.deepcopy(tokenized_sentence)\n",
    "            \n",
    "            inputs.append(tokenized_sentence_masked)\n",
    "            labels.append(tokenized_sentence['input_ids'])\n",
    "            tokenized_sentence = tokenized\n",
    "    \n",
    "    return inputs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(inputs, labels, idx,batch_size=4):\n",
    "    new_inputs = copy.deepcopy(inputs[idx])\n",
    "    new_labels = copy.deepcopy(labels[idx])\n",
    "    \n",
    "    for temp_input, temp_label in zip(inputs[idx+1:idx+batch_size],labels[idx+1:idx+batch_size]):\n",
    "        for key in temp_input.keys():\n",
    "            new_inputs[key] = torch.vstack((new_inputs[key], temp_input[key]))\n",
    "        new_labels = torch.vstack((new_labels,temp_label))\n",
    "    \n",
    "    return new_inputs, new_labels\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f6721-75e9-466e-88c9-92086fb193a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "book_idx = 0\n",
    "total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dddc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_total_steps = 6000\n",
    "num_warmup_steps = int(num_total_steps*0.2)\n",
    "updates_count = 0\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555b1b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "batch_size = 4\n",
    "update_size = 256 #actuall batch_size after considering gradient accumulation\n",
    "count_global = updates_count*update_size\n",
    "count_local = 1\n",
    "accumulation = update_size/batch_size\n",
    "labels = None\n",
    "inputs = None\n",
    "\n",
    "while True:\n",
    "    model.train()\n",
    "    if updates_count > num_total_steps: #break out of the loop when learing rate hits 0\n",
    "        break\n",
    "    count_local = 1\n",
    "    training_examples = 0\n",
    "    for _ in tqdm(range(500)): #so every 500 speeches we have evaluation\n",
    "        if book_idx >= 15750: #set to max number of documents\n",
    "            book_idx = 0\n",
    "        try:\n",
    "            book = read_book(books_list[book_idx])\n",
    "        except Exception as e:\n",
    "            print(e) #just incase \n",
    "            \n",
    "        book_idx += 1    \n",
    "        sentences = split_into_sentences(book)\n",
    "        if len(sentences) == 0:\n",
    "            continue  #skip when book failed to load\n",
    "            \n",
    "        if labels == None:\n",
    "            inputs,labels = tokenize_sentences(sentences)\n",
    "            training_examples += len(labels)\n",
    "        else:\n",
    "            new_inputs, new_labels = tokenize_sentences(sentences)\n",
    "            training_examples += len(new_labels)\n",
    "            labels.extend(new_labels)\n",
    "            inputs.extend(new_inputs)\n",
    "            \n",
    "        num_backwards = len(labels)//update_size\n",
    "        \n",
    "        if num_backwards > 0:\n",
    "            idx = 0\n",
    "            \n",
    "            for _ in range(int(num_backwards*accumulation)):\n",
    "                temp_input,temp_label = batch_data(inputs, labels, idx, batch_size)\n",
    "                with autocast():\n",
    "                    outputs = model(**temp_input, labels=temp_label)\n",
    "                    loss = outputs.loss/accumulation\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                scaler.scale(loss).backward()\n",
    "                count_global += 1\n",
    "                count_local += 1\n",
    "                \n",
    "                if int((count_global+1) % accumulation) == 0:\n",
    "\n",
    "                    updates_count += 1\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    if (updates_count+1) % 1000 == 0: #save checkpoint every ____ updates\n",
    "                        mean_loss = accumulation * total_loss/count_local\n",
    "                        print(\"Saving model, {} steps, mean loss is {}\".format(updates_count,mean_loss))\n",
    "                        checkpoint = { \n",
    "                            'updates_count': updates_count,\n",
    "                            'model': model.state_dict(),\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                            'scheduler': scheduler.state_dict(),\n",
    "                            \"book_idx\" : book_idx,\n",
    "                            'total_loss':total_loss}\n",
    "                            \n",
    "                        torch.save(checkpoint, 'checkpoints/checkpoint_{}_{}.pth'.format(book_idx,updates_count))\n",
    "                        torch.save(model.state_dict(), \"models/pytorch_model_{}_{}.bin\".format(book_idx,updates_count))\n",
    "\n",
    "\n",
    "                idx += batch_size\n",
    "\n",
    "        inputs = inputs[num_backwards*update_size:]\n",
    "        labels = labels[num_backwards*update_size:]\n",
    "        \n",
    "    mean_loss = accumulation*total_loss/count_global\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    text1 = \"Number of sentences: {}, Mean Loss: {}, Number of backprops: {}, Forwards: {}, Last book: {}\".format(training_examples, mean_loss, updates_count,count_local,books_list[book_idx])\n",
    "    text2 = \"-------------------------------------------\"\n",
    "    \n",
    "    with open(\"generated.txt\",\"a\", encoding = \"utf-8\") as f:\n",
    "        f.write(text1)\n",
    "        for i in range(5):\n",
    "            sample_outputs = model.generate(\n",
    "                                            bos_token_id=np.random.randint(1,50256),\n",
    "                                            do_sample=True,   \n",
    "                                            top_k=50, \n",
    "                                            max_length = 128,\n",
    "                                            top_p=0.95, \n",
    "                                            num_return_sequences=1\n",
    "                                        )\n",
    "\n",
    "            for _, sample_output in enumerate(sample_outputs):\n",
    "                generated_text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "                text = \"{}: {} \\n lenght of generated text: {} \".format(i,generated_text,len(generated_text))  \n",
    "                f.write(text)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        f.write(text2)\n",
    "        f.close()\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "checkpoint = { \n",
    "    'updates_count': updates_count,\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    \"book_idx\" : book_idx}\n",
    "torch.save(checkpoint, 'checkpoints/checkpoint_{}_{}.pth'.format(book_idx,updates_count))\n",
    "torch.save(model.state_dict(), \"models/pytorch_model_{}_{}.bin\".format(book_idx,updates_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef5cd4-b6b0-4c7d-8963-e64d649ad301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
